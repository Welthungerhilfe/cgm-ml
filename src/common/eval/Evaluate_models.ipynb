{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a trained model\n",
    "\n",
    "## Setup\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "import glob2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent / 'src'))\n",
    "\n",
    "from eval_utils import calculate_performance, CODE_TO_SCANTYPE, REPO_DIR\n",
    "from config import CONFIG\n",
    "from preprocessing import preprocess_targets, preprocess_depthmap, tf_load_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the  model to be evaluated from workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Workspace.from_config()\n",
    "\n",
    "# checkboxes = []\n",
    "# for experiment_name, experiment in workspace.experiments.items():\n",
    "#     checkbox = widgets.Checkbox(value=False, description=experiment_name)\n",
    "#     display(checkbox)\n",
    "#     checkboxes.append(checkbox)\n",
    "\n",
    "# Get the selected experiments.\n",
    "# selected_experiments = [checkbox.description for checkbox in checkboxes if checkbox.value]\n",
    "\n",
    "selected_experiments = [\"q3-depthmap-plaincnn-height-95k\"]\n",
    "RUN_ID = 'q3-depthmap-plaincnn-height-95k_1597988908_42c4ef33'  # Run3\n",
    "OUTPUT_DIR = 'data/logs/q3-depthmap-plaincnn-height-95k/run_03/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the models on your local system for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get folder.\n",
    "# temp_path = \"logs\"\n",
    "# if os.path.exists(temp_path):\n",
    "#     shutil.rmtree(temp_path)\n",
    "# os.mkdir(temp_path)\n",
    "\n",
    "# # Download logs of all completed runs\n",
    "# for selected_experiment in selected_experiments:\n",
    "#     print(f\"Experiment: {selected_experiment}\")\n",
    "#     experiment = workspace.experiments.get(selected_experiment)\n",
    "#     for run_index, run in enumerate(list(experiment.get_runs())[::-1]):\n",
    "#         log_path = os.path.join(temp_path, experiment.name, \"run_{:02d}\".format(run_index + 1))\n",
    "#         if run.id == RUN_ID:\n",
    "#             print(\"Run: {}\".format(run_index + 1))\n",
    "#             run.download_files(output_directory=OUTPUT_DIR, output_paths=None, batch_size=100, append_prefix=False)\n",
    "# #             run.download_files(prefix=\".h5\", output_directory=log_path, output_paths=None, batch_size=100, append_prefix=False)\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = 'evaluation_95k_30082020/q3-depthmap-plaincnn-height-100-95k/run_03/outputs/best_model.h5'\n",
    "MODEL_PATH = str(REPO_DIR / \"data/outputs/best_model_Run3_nodropout.h5\")\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "# summarize model.\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a sample from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = '../testdepthmap1/1585551618-hlby208u8z/pc_1585551618-hlby208u8z_1593156356859_100_000.p'\n",
    "paths = REPO_DIR / \"data/anon-depthmap-testset/scans/1585551618-hlby208u8z/100/pc_1585551618-hlby208u8z_1593156356859_100_000.p\"\n",
    "\n",
    "depthmap, targets = pickle.load(open(paths, \"rb\"))\n",
    "depthmap = preprocess_depthmap(depthmap)\n",
    "depthmap = depthmap/depthmap.max()\n",
    "print(\"depthmap_max:\",depthmap.max())\n",
    "depthmap = tf.image.resize(depthmap, (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH))\n",
    "targets = preprocess_targets(targets, CONFIG.TARGET_INDEXES)\n",
    "depthmap.set_shape((CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1))\n",
    "# targets.set_shape((len(targets_indices,)))\n",
    "plt.imshow(np.squeeze(depthmap), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the samples from testset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    depthmap, targets = pickle.load(open(path, \"rb\"))\n",
    "    depthmap = preprocess_depthmap(depthmap)\n",
    "    depthmap = depthmap / CONFIG.NORMALIZATION_VALUE\n",
    "    depthmap = tf.image.resize(depthmap, (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH))\n",
    "    targets = preprocess_targets(targets, CONFIG.TARGET_INDEXES)\n",
    "    depthmap.set_shape((CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1))\n",
    "    return depthmap,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To use the dataset, you can:\n",
    "- mount the dataset\n",
    "- use datastore (blob storage)\n",
    "- download the dataset\n",
    "\n",
    "Choose your preferred way and make sure to adjust the absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = '/mnt/depthmap/depthmap_testset/scans/*/*/'\n",
    "DATASET_PATH = str(REPO_DIR / \"data/anon-depthmap-testset/scans/*/*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = glob2.glob(DATASET_PATH); prediction_folder[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = prediction_folder[:10]  # reduce size for DEBUG speed\n",
    "\n",
    "predictions = []\n",
    "for qrcode in tqdm(prediction_folder):\n",
    "    depthmaps_pred = []\n",
    "    labels = []\n",
    "    depthfiles = []\n",
    "    depthmaps = glob2.glob(qrcode + '/*.p')\n",
    "    for files in depthmaps:\n",
    "        depths, targets = preprocess(files)\n",
    "        depthmaps_pred.append(depths)\n",
    "        labels.append(targets)\n",
    "        depthfiles.append(files)\n",
    "    files_to_predict = tf.stack(depthmaps_pred)\n",
    "    inference = model.predict(files_to_predict)\n",
    "    predictions.append([qrcode, depthfiles,np.squeeze(inference), labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## putting the predictions in a dataframe\n",
    "df = pd.DataFrame([])\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    label = np.array(predictions[i][3]).flatten()\n",
    "    data = pd.DataFrame({\n",
    "        'qrcode':predictions[i][0],\n",
    "        'artifacts': predictions[i][1],\n",
    "        'predicted':predictions[i][2],\n",
    "        'GT':label,\n",
    "    })\n",
    "    df = df.append(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qrcode(row):\n",
    "    qrc = row['artifacts'].split('/')[-3]\n",
    "    return qrc\n",
    "\n",
    "def extract_scantype(row):\n",
    "    \"\"\"https://dev.azure.com/cgmorg/ChildGrowthMonitor/_wiki/wikis/ChildGrowthMonitor.wiki/15/Codes-for-Pose-and-Scan-step\"\"\"\n",
    "    scans = row['artifacts'].split('/')[-2]\n",
    "    return scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qrcode'] = df.apply(extract_qrcode, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artifacts'].iloc[1]  # sample of how the artifacts path looks like for me, modify it accordingly to suit your path dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['qrcode'].unique()) ## total number of scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scantype'] = df.apply(extract_scantype, axis=1)\n",
    "df['scantype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the results of artifacts by qrcode and scantype by taking mean across the same scantype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = df.groupby(['qrcode', 'scantype']).mean()\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error between predicted and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgerror(row):\n",
    "    difference = row['GT'] - row['predicted']\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE['error'] = MAE.apply(avgerror, axis=1)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## froming the unique name for the index values\n",
    "model_name = 'q3-depthmap-plaincnn-height-100-95k'\n",
    "run_no ='_front_run_03'\n",
    "complete_name = model_name + run_no; complete_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating accuracies across the scantypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for code in CODE_TO_SCANTYPE.keys():\n",
    "    df = calculate_performance(code, MAE)\n",
    "    full_model_name = complete_name + CODE_TO_SCANTYPE[code]\n",
    "    df.rename(index={0:full_model_name}, inplace=True)\n",
    "    display(HTML(df.to_html()))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining the results for all accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(dfs)\n",
    "result.index.name = 'Model_Scantype'\n",
    "result = result.round(2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model results in csv file\n",
    "CSV_OUT_PATH = REPO_DIR / 'data' / 'eval' / RUN_ID / 'result.csv'\n",
    "Path(CSV_OUT_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "result.to_csv(CSV_OUT_PATH, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
