{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a trained model\n",
    "\n",
    "Setup for tqdm widgets: https://ipywidgets.readthedocs.io/en/stable/user_install.html#installing-the-jupyterlab-extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.run import Run\n",
    "import glob2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent / 'src'))\n",
    "from eval_utils import calculate_performance, CODE_TO_SCANTYPE, CONFIG, MODEL_CKPT_FILENAME, REPO_DIR, preprocess_targets, preprocess_depthmap, preprocess, extract_qrcode, extract_scantype, avgerror\n",
    "\n",
    "sys.path.append(str(REPO_DIR / 'src/common/model_utils'))\n",
    "from preprocessing_multiartifact2 import create_multiartifact_samples\n",
    "from preprocessing_multiartifact2_multiartifact import create_multiartifact_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(REPO_DIR)\n",
    "DATA_DIR = REPO_DIR / 'data' if Run.get_context().id.startswith(\"OfflineRun\") else Path(\".\")\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the  model to be evaluated from workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Workspace.from_config()\n",
    "\n",
    "# RUN_ID = 'q3-depthmapmultiartifactlatefusion-plaincnn-height-95k_1614066635_8c470f0a'\n",
    "# RUN_NUMBER = 4\n",
    "\n",
    "RUN_ID = 'q3-depthmapmultiartifactlatefusion-plaincnn-height-95k_1614177517_ecd7b6e2'\n",
    "RUN_NUMBER = 6\n",
    "\n",
    "\n",
    "EXPERIMENT = \"_\".join(RUN_ID.split('_')[:-2])\n",
    "OUTPUT_DIR = f'data/logs/q3-depthmapmultiartifact-plaincnn-height-95k/run_{RUN_NUMBER}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the models on your local system for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "print(f\"Downloading model from {RUN_ID}\")\n",
    "previous_experiment = Experiment(workspace=workspace, name=EXPERIMENT)\n",
    "previous_run = Run(previous_experiment, RUN_ID)\n",
    "model_fpath = DATA_DIR / \"pretrained\" / RUN_ID\n",
    "previous_run.download_files(f\"outputs/{MODEL_CKPT_FILENAME}\", model_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with local model\n",
    "# model_fpath = DATA_DIR / 'outputs' / MODEL_CKPT_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f'{model_fpath}/outputs/{MODEL_CKPT_FILENAME}')\n",
    "# summarize model.\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a sample from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean: anon-depthmap-testset\n",
    "# DATASET_DIR = DATA_DIR / \"anon-depthmap-testset\" / \"scans\"  # locally\n",
    "# DATASET_DIR = Path('/mnt/datasets/depthmap_testset') / \"scans\"  # on VM\n",
    "\n",
    "# Unclean: anon-realtime-testdata\n",
    "# DATASET_DIR = DATA_DIR / \"anon-realtime-testdata\" / \"depthmaps\" # locally\n",
    "DATASET_DIR = Path('/mnt/datasets/realtime_evaluation') / \"depthmaps\" # on VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = DATASET_DIR / \"1585015607-01sk32pthg/100/pc_1585015607-01sk32pthg_1591875909425_100_000.p\"\n",
    "\n",
    "depthmap, targets = pickle.load(open(paths, \"rb\"))\n",
    "depthmap = preprocess_depthmap(depthmap)\n",
    "depthmap = depthmap / depthmap.max()\n",
    "print(\"depthmap_max:\", depthmap.max())\n",
    "depthmap = tf.image.resize(depthmap, (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH))\n",
    "targets = preprocess_targets(targets, CONFIG.TARGET_INDEXES)\n",
    "depthmap.set_shape((CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1))\n",
    "# targets.set_shape((len(targets_indices,)))\n",
    "plt.imshow(np.squeeze(depthmap), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To use the dataset, you can:\n",
    "- mount the dataset\n",
    "- use datastore (blob storage)\n",
    "- download the dataset\n",
    "\n",
    "Choose your preferred way and make sure to adjust the absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = str(DATASET_DIR / \"*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrcode_paths = glob2.glob(DATASET_PATH); \n",
    "print(len(qrcode_paths))\n",
    "qrcode_paths = qrcode_paths# [:100]  # reduce size for DEBUG speed\n",
    "qrcode_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_paths = create_multiartifact_samples(qrcode_paths, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(samples_paths), samples_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sample_paths in tqdm(samples_paths):\n",
    "    depthmap, targets = create_multiartifact_sample(sample_paths,\n",
    "                                                    CONFIG.NORMALIZATION_VALUE,\n",
    "                                                    CONFIG.IMAGE_TARGET_HEIGHT,\n",
    "                                                    CONFIG.IMAGE_TARGET_WIDTH,\n",
    "                                                    CONFIG.TARGET_INDEXES)\n",
    "    depthmaps = tf.stack([depthmap])\n",
    "    \n",
    "    pred = model.predict(depthmaps)\n",
    "    \n",
    "    predictions.append([sample_paths[0], float(np.squeeze(pred)), targets[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 minutes for all 1745 scans' predictions\n",
    "\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to dataframe\n",
    "df = pd.DataFrame(predictions, columns=['artifacts', 'predicted', 'GT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artifacts'].iloc[1]  # sample of how the artifacts path looks like for me, modify it accordingly to suit your path dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scantype'] = df.apply(extract_scantype, axis=1)\n",
    "df['qrcode'] = df.apply(extract_qrcode, axis=1)\n",
    "df['scantype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['qrcode'].unique()) ## total number of scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the results of artifacts by qrcode and scantype by taking mean across the same scantype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = df.groupby(['qrcode', 'scantype']).mean()\n",
    "# MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error between predicted and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE['error'] = MAE.apply(avgerror, axis=1)\n",
    "# MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracies across the scantypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for code in CODE_TO_SCANTYPE.keys():\n",
    "    # unique name for the index values\n",
    "    model_name = 'q3-depthmap-plaincnn-height-100-95k'\n",
    "    run_no = f'{CODE_TO_SCANTYPE[code]}_run_{RUN_NUMBER}'\n",
    "    complete_name = EXPERIMENT + run_no; complete_name\n",
    "\n",
    "    df_out = calculate_performance(code, MAE)\n",
    "    full_model_name = complete_name + CODE_TO_SCANTYPE[code]\n",
    "    df_out.rename(index={0:full_model_name}, inplace=True)\n",
    "    df_out = df_out.round(2)\n",
    "    display(HTML(df_out.to_html()))\n",
    "    dfs.append(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artifacts'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the results for all accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(dfs)\n",
    "result.index.name = 'Model_Scantype'\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model results in csv file\n",
    "CSV_OUT_PATH = DATA_DIR / 'eval' / RUN_ID / 'result.csv'\n",
    "Path(CSV_OUT_PATH.parent).mkdir(parents=True, exist_ok=True)\n",
    "result.to_csv(CSV_OUT_PATH, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "cgm_qa_pipeline"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
