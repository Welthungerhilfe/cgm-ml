{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a trained model\n",
    "\n",
    "## Setup\n",
    "\n",
    "```\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "import glob2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import REPO_DIR\n",
    "print(f\"REPO_DIR: {REPO_DIR}\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(REPO_DIR / \"src/models/CNNDepthMap/CNNDepthMap-height/q3-depthmapmultiartifactlatefusion-plaincnn-height/src\"))\n",
    "from config import CONFIG\n",
    "from preprocessing import preprocess_targets, preprocess_depthmap, tf_load_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the  model to be evaluated from workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Workspace.from_config()\n",
    "\n",
    "# checkboxes = []\n",
    "# for experiment_name, experiment in workspace.experiments.items():\n",
    "#     checkbox = widgets.Checkbox(value=False, description=experiment_name)\n",
    "#     display(checkbox)\n",
    "#     checkboxes.append(checkbox)\n",
    "\n",
    "# Get the selected experiments.\n",
    "# selected_experiments = [checkbox.description for checkbox in checkboxes if checkbox.value]\n",
    "\n",
    "selected_experiments = [\"q3-depthmap-plaincnn-height-95k\"]\n",
    "RUN_ID = 'q3-depthmap-plaincnn-height-95k_1597988908_42c4ef33'  # Run3\n",
    "OUTPUT_DIR = 'data/logs/q3-depthmap-plaincnn-height-95k/run_03/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the models on your local system for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get folder.\n",
    "temp_path = \"logs\"\n",
    "if os.path.exists(temp_path):\n",
    "    shutil.rmtree(temp_path)\n",
    "os.mkdir(temp_path)\n",
    "\n",
    "# Download logs of all completed runs\n",
    "for selected_experiment in selected_experiments:\n",
    "    print(f\"Experiment: {selected_experiment}\")\n",
    "    experiment = workspace.experiments.get(selected_experiment)\n",
    "    for run_index, run in enumerate(list(experiment.get_runs())[::-1]):\n",
    "        log_path = os.path.join(temp_path, experiment.name, \"run_{:02d}\".format(run_index + 1))\n",
    "        if run.id == RUN_ID:\n",
    "            print(\"Run: {}\".format(run_index + 1))\n",
    "            run.download_files(output_directory=OUTPUT_DIR, output_paths=None, batch_size=100, append_prefix=False)\n",
    "#             run.download_files(prefix=\".h5\", output_directory=log_path, output_paths=None, batch_size=100, append_prefix=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = 'evaluation_95k_30082020/q3-depthmap-plaincnn-height-100-95k/run_03/outputs/best_model.h5'\n",
    "MODEL_PATH = str(REPO_DIR / \"data/outputs/best_model_Run3_nodropout.h5\")\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "# summarize model.\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_target_height = CONFIG.IMAGE_TARGET_HEIGHT\n",
    "image_target_width = CONFIG.IMAGE_TARGET_WIDTH\n",
    "\n",
    "# def tf_load_pickle(path):\n",
    "#     def py_load_pickle(path):\n",
    "#         depthmap, targets = pickle.load(open(path, \"rb\"))\n",
    "#         depthmap = preprocess_depthmap(depthmap)\n",
    "#         depthmap = depthmap/depthmap.max()\n",
    "#         depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "#         targets = preprocess_targets(targets, targets_indices)\n",
    "#         return depthmap, targets\n",
    "    \n",
    "#     depthmap, targets = tf.py_function(py_load_pickle, [path], [tf.float32, tf.float32])\n",
    "#     depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "#     targets.set_shape((len(targets_indices,)))\n",
    "#     return depthmap, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_targets(targets, targets_indices):\n",
    "#     if targets_indices is not None:\n",
    "#         targets = targets[targets_indices]\n",
    "#     return targets.astype(\"float32\")\n",
    "\n",
    "# def preprocess_depthmap(depthmap):\n",
    "#     # TODO here be more code.\n",
    "#     return depthmap.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_indices = CONFIG.TARGET_INDEXES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a sample from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = '../testdepthmap1/1585551618-hlby208u8z/pc_1585551618-hlby208u8z_1593156356859_100_000.p'\n",
    "paths = REPO_DIR / \"data/anon-depthmap-testset/scans/1585551618-hlby208u8z/100/pc_1585551618-hlby208u8z_1593156356859_100_000.p\"\n",
    "\n",
    "depthmap, targets = pickle.load(open(paths, \"rb\"))\n",
    "depthmap = preprocess_depthmap(depthmap)\n",
    "depthmap = depthmap/depthmap.max()\n",
    "print(\"depthmap_max:\",depthmap.max())\n",
    "depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "targets = preprocess_targets(targets, targets_indices)\n",
    "depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "# targets.set_shape((len(targets_indices,)))\n",
    "plt.imshow(np.squeeze(depthmap));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the samples from testset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    depthmap, targets = pickle.load(open(path, \"rb\"))\n",
    "    depthmap = preprocess_depthmap(depthmap)\n",
    "#     depthmap = depthmap/depthmap.max()\n",
    "    depthmap = depthmap / 7.5\n",
    "    depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "    targets = preprocess_targets(targets, targets_indices)\n",
    "    depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "    return depthmap,targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount the dataset and provide the absolute path\n",
    "\n",
    "# prediction_folder = glob2.glob('/mnt/depthmap/depthmap_testset/scans/*/*/') \n",
    "prediction_folder = glob2.glob(str(REPO_DIR / \"data/anon-depthmap-testset/scans/*/*/\")); prediction_folder[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder = prediction_folder[:10]  # DEBUG SPEED\n",
    "\n",
    "predictions = []\n",
    "for qrcode in tqdm(prediction_folder):\n",
    "    depthmaps_pred = []\n",
    "    labels = []\n",
    "    depthfiles = []\n",
    "    depthmaps = glob2.glob(qrcode + '/*.p')\n",
    "    for files in depthmaps:\n",
    "        depths, targets = preprocess(files)\n",
    "        depthmaps_pred.append(depths)\n",
    "        labels.append(targets)\n",
    "        depthfiles.append(files)\n",
    "    files_to_predict = tf.stack(depthmaps_pred)\n",
    "    inference = model.predict(files_to_predict)\n",
    "    predictions.append([qrcode, depthfiles,np.squeeze(inference), labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## putting the predictions in a dataframe\n",
    "df = pd.DataFrame([])\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    label = np.array(predictions[i][3]).flatten()\n",
    "    data = pd.DataFrame({\n",
    "        'qrcode':predictions[i][0],\n",
    "        'artifacts': predictions[i][1],\n",
    "        'predicted':predictions[i][2],\n",
    "        'GT':label,\n",
    "    })\n",
    "    df = df.append(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qrcode(row):\n",
    "    qrc = row['artifacts'].split('/')[-3]\n",
    "    return qrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qrcode'] = df.apply(extract_qrcode, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artifacts'].iloc[1] # sample of how the artifacts path looks like for me, modify it accordingly to suit your path dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['qrcode'].unique()) ## total number of scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scantype(row):\n",
    "    scans = row['artifacts'].split('/')[-2]\n",
    "    return scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"## keys for different scan type\n",
    "# \"- 100 - standing front scan\\\"\n",
    "# \"\"- 101 - standing 360 scan\\\",\n",
    "# \"\\\"- 102 - standing back scan\\\",\\n\",\n",
    "# \"\\\"- 200 - lying front scan\\\",\\n\",\n",
    "# \"\\\"- 201 - lying side scan\\\",\\n\",\n",
    "# \"\\\"- 202 - lying back scan\\\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scantype'] = df.apply(extract_scantype, axis=1)\n",
    "df['scantype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the results of artifacts by qrcode and scantype by taking mean across the same scantype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = df.groupby(['qrcode', 'scantype']).mean()\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error between predicted and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgerror(row):\n",
    "    difference = row['GT'] - row['predicted']\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE['error'] = MAE.apply(avgerror, axis=1)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## error margin on various ranges\n",
    "evaluation_accuracies = [.2,.4,.8,1.2,2,2.5,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## froming the unique name for the index values\n",
    "model_name = 'q3-depthmap-plaincnn-height-100-95k'\n",
    "run_no ='_front_run_03'\n",
    "complete_name = model_name + run_no; complete_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating accuracies across the scantypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_360'\n",
    "full_model_name = complete_name + Scan\n",
    "rot = MAE.iloc[MAE.index.get_level_values('scantype') == '101']\n",
    "accuracy_list = []\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_rot = rot[(rot['error']<=acc) & (rot['error']>=-acc)]\n",
    "    rot_accuracy = len(predicted_rot)/len(rot)*100\n",
    "    print(\"total accuracy {} for 360:{}\".format(acc,rot_accuracy))\n",
    "    accuracy_list.append(rot_accuracy)\n",
    "rotdata = pd.DataFrame(accuracy_list)\n",
    "rotdata = rotdata.T\n",
    "rotdata.columns = evaluation_accuracies\n",
    "rotdata.rename(index={0:full_model_name}, inplace=True)\n",
    "rotdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_back'\n",
    "full_model_name = complete_name + Scan\n",
    "back = MAE.iloc[MAE.index.get_level_values('scantype') == '102']\n",
    "print(len(back))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_back = back[(back['error']<=acc) & (back['error']>=-acc)]\n",
    "    back_accuracy = len(predicted_back)/len(back)*100\n",
    "    print(\"total accuracy {} for back:{}\".format(acc,back_accuracy))\n",
    "    accuracy_list.append(back_accuracy)\n",
    "backdata =pd.DataFrame(accuracy_list)\n",
    "backdata =backdata.T\n",
    "backdata.columns = evaluation_accuracies\n",
    "backdata.rename(index={0:full_model_name}, inplace=True)\n",
    "backdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_front'\n",
    "full_model_name = complete_name + Scan\n",
    "front= MAE.iloc[MAE.index.get_level_values('scantype') == '100']\n",
    "print(\"total front samples:\",len(front))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_front = front[(front['error']<=acc) & (front['error']>=-acc)]\n",
    "    front_accuracy = len(predicted_front)/len(front)*100\n",
    "    print(\"total accuracy {} for front:{}\".format(acc,front_accuracy))\n",
    "    accuracy_list.append(front_accuracy)\n",
    "frontdata =pd.DataFrame(accuracy_list)\n",
    "frontdata =frontdata.T\n",
    "frontdata.columns = evaluation_accuracies\n",
    "frontdata.rename(index={0:full_model_name}, inplace=True)\n",
    "frontdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_lyingfront'\n",
    "full_model_name = complete_name + Scan\n",
    "lyingfront= MAE.iloc[MAE.index.get_level_values('scantype') == '200']\n",
    "print(\"total lyingfront samples:\",len(lyingfront))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_front = lyingfront[(lyingfront['error']<=acc) & (lyingfront['error']>=-acc)]\n",
    "    front_accuracy = len(predicted_front)/len(lyingfront)*100\n",
    "    print(\"total accuracy {} for lyingfront:{}\".format(acc,front_accuracy))\n",
    "    accuracy_list.append(front_accuracy)\n",
    "lyingfrontdata =pd.DataFrame(accuracy_list)\n",
    "lyingfrontdata =lyingfrontdata.T\n",
    "lyingfrontdata.columns = evaluation_accuracies\n",
    "lyingfrontdata.rename(index={0:full_model_name}, inplace=True)\n",
    "lyingfrontdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_lyingrot'\n",
    "full_model_name = complete_name + Scan\n",
    "lyingrot= MAE.iloc[MAE.index.get_level_values('scantype') == '201']\n",
    "print(\"total lyingrot samples:\",len(lyingrot))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_front = lyingrot[(lyingrot['error']<=acc) & (lyingrot['error']>=-acc)]\n",
    "    front_accuracy = len(predicted_front)/len(lyingrot)*100\n",
    "    print(\"total accuracy {} for lyingfront:{}\".format(acc,front_accuracy))\n",
    "    accuracy_list.append(front_accuracy)\n",
    "lyingrotdata =pd.DataFrame(accuracy_list)\n",
    "lyingrotdata =lyingrotdata.T\n",
    "lyingrotdata.columns = evaluation_accuracies\n",
    "lyingrotdata.rename(index={0:full_model_name}, inplace=True)\n",
    "lyingrotdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_lyingback'\n",
    "full_model_name = complete_name + Scan\n",
    "lyingback= MAE.iloc[MAE.index.get_level_values('scantype') == '202']\n",
    "print(\"total lyingback samples:\",len(lyingback))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_back = lyingback[(lyingback['error']<=acc) & (lyingback['error']>=-acc)]\n",
    "    back_accuracy = len(predicted_back)/len(lyingback)*100\n",
    "    print(\"total accuracy {} for lyingback:{}\".format(acc,back_accuracy))\n",
    "    accuracy_list.append(back_accuracy)\n",
    "lyingbackdata =pd.DataFrame(accuracy_list)\n",
    "lyingbackdata =lyingbackdata.T\n",
    "lyingbackdata.columns = evaluation_accuracies\n",
    "lyingbackdata.rename(index={0:full_model_name}, inplace=True)\n",
    "lyingbackdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining the results for all accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = pd.concat([frontdata, rotdata, backdata, lyingfrontdata, lyingrotdata, lyingbackdata])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.index.name = 'Model_Scantype'\n",
    "# alldata.set_names('Model', level=None, inplace=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model results in csv file\n",
    "result.to_csv('evaluation_95k_30082020/q3-depthmap-plaincnn-height-95k/run_03.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results =pd.concat([run_03,run_04,front])\n",
    "complete_results = complete_results.round(2) ## saving the results upto two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results.to_csv('Model_results/95k_data_v0/results.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
