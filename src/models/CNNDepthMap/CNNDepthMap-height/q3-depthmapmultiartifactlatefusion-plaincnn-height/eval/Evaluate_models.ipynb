{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.dataset import Dataset\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import glob2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the  model to be evaluated from workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = Workspace.from_config()\n",
    "workspace\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "checkboxes = []\n",
    "\n",
    "for experiment_name, experiment in workspace.experiments.items():\n",
    "    checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description=experiment_name\n",
    "    )\n",
    "    display(checkbox)\n",
    "    checkboxes.append(checkbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the models on your local system for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Get the selected experiments.\n",
    "selected_experiments = []\n",
    "for checkbox in checkboxes:\n",
    "    if checkbox.value == True:\n",
    "        selected_experiments.append(checkbox.description)\n",
    "\n",
    "# Get folder.\n",
    "temp_path = \"logs\"\n",
    "if os.path.exists(temp_path):\n",
    "    shutil.rmtree(temp_path)\n",
    "os.mkdir(temp_path)\n",
    "\n",
    "# For each selected experiment download logs of all completed runs.\n",
    "for selected_experiment in selected_experiments:\n",
    "    print(\"Experiment: {}\".format(selected_experiment))\n",
    "    experiment = workspace.experiments.get(selected_experiment)\n",
    "    for run_index, run in enumerate(list(experiment.get_runs())[::-1]):\n",
    "        log_path = os.path.join(temp_path, experiment.name, \"run_{:02d}\".format(run_index + 1))\n",
    "#         if run.status == \"Failed\" and run.id == 'q3-depthmap-plaincnn-height_1595403832_d4528115':\n",
    "        if  run.id == 'q3-depthmap-plaincnn-height-95k_1597988908_42c4ef33':\n",
    "            print(\"Run: {}\".format(run_index + 1))\n",
    "            run.download_files(output_directory='logs/q3-depthmap-plaincnn-height-95k/run_03/', output_paths=None, batch_size=100, append_prefix=False)\n",
    "#             run.download_files(prefix=\".h5\", output_directory=log_path, output_paths=None, batch_size=100, append_prefix=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('evaluation_95k_30082020/q3-depthmap-plaincnn-height-100-95k/run_03/outputs/best_model.h5')\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_target_height = 240\n",
    "image_target_width =180\n",
    "\n",
    "def tf_load_pickle(path):\n",
    "    def py_load_pickle(path):\n",
    "        depthmap, targets = pickle.load(open(path, \"rb\"))\n",
    "        depthmap = preprocess_depthmap(depthmap)\n",
    "        depthmap = depthmap/depthmap.max()\n",
    "        depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "        targets = preprocess_targets(targets, targets_indices)\n",
    "        return depthmap, targets\n",
    "    \n",
    "    depthmap, targets = tf.py_function(py_load_pickle, [path], [tf.float32, tf.float32])\n",
    "    depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "    targets.set_shape((len(targets_indices,)))\n",
    "    return depthmap, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, targets_indices):\n",
    "    if targets_indices is not None:\n",
    "        targets = targets[targets_indices]\n",
    "    return targets.astype(\"float32\")\n",
    "\n",
    "def preprocess_depthmap(depthmap):\n",
    "    # TODO here be more code.\n",
    "    return depthmap.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_indices = [0] # height =0 , weight =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a sample from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = '../testdepthmap1/1585551618-hlby208u8z/pc_1585551618-hlby208u8z_1593156356859_100_000.p'\n",
    "depthmap, targets = pickle.load(open(paths, \"rb\"))\n",
    "depthmap = preprocess_depthmap(depthmap)\n",
    "depthmap = depthmap/depthmap.max()\n",
    "print(\"depthmap_max:\",depthmap.max())\n",
    "depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "targets = preprocess_targets(targets, targets_indices)\n",
    "depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "# targets.set_shape((len(targets_indices,)))\n",
    "plt.imshow(np.squeeze(depthmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the samples from testset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    depthmap, targets = pickle.load(open(path, \"rb\"))\n",
    "    depthmap = preprocess_depthmap(depthmap)\n",
    "#     depthmap = depthmap/depthmap.max()\n",
    "    depthmap = depthmap/7.5\n",
    "    depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "    targets = preprocess_targets(targets, targets_indices)\n",
    "    depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "    return depthmap,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "predictions =[]\n",
    "prediction_folder = glob2.glob('/mnt/depthmap/depthmap_testset/scans/*/*/') ## mount the dataset and provide the absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for qrcode in tqdm(prediction_folder):\n",
    "    depthmaps_pred = []\n",
    "    labels = []\n",
    "    depthfiles = []\n",
    "    depthmaps = glob2.glob(qrcode+'*.p')\n",
    "    for files in depthmaps:\n",
    "        depths,targets = preprocess(files)\n",
    "        depthmaps_pred.append(depths)\n",
    "        labels.append(targets)\n",
    "        depthfiles.append(files)\n",
    "    files_to_predict  =tf.stack(depthmaps_pred)\n",
    "    inference = model.predict(files_to_predict)\n",
    "    predictions.append([qrcode,depthfiles,np.squeeze(inference),labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## putting the predictions in a dataframe\n",
    "df = pd.DataFrame([])\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    label = np.array(predictions[i][3])\n",
    "    label =label.flatten()\n",
    "    data =pd.DataFrame({'qrcode':predictions[i][0],'artifacts': predictions[i][1],'predicted':predictions[i][2],'GT':label})\n",
    "    df = df.append(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to extract the qrcode from the artifacts \n",
    "def qrcode(row):\n",
    "    qrc = row['artifacts'].split('/')[5]\n",
    "    return qrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qrcode'] = df.apply(qrcode,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artifacts'].iloc[1] # sample of how the artifacts path looks like for me, modify it accordingly to suit your path dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['qrcode'].unique()) ## total number of scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to extract the scantype of artifacts and display the number \n",
    "def scantype(row):\n",
    "    scans = row['artifacts'].split('/')[6]\n",
    "    return scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"## keys for different scan type\n",
    "\"- 100 - standing front scan\\\"\n",
    "\"\"- 101 - standing 360 scan\\\",\n",
    "\"\\\"- 102 - standing back scan\\\",\\n\",\n",
    "\"\\\"- 200 - lying front scan\\\",\\n\",\n",
    "\"\\\"- 201 - lying side scan\\\",\\n\",\n",
    "\"\\\"- 202 - lying back scan\\\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scantype'] = df.apply(scantype, axis=1)\n",
    "df['scantype'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group the results of artifacts by qrcode and scantype by taking mean across the same scantype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = df.groupby(['qrcode','scantype']).mean()\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error between predicted and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgerror(row):\n",
    "    difference =row['GT'] - row['predicted']\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE['error'] = MAE.apply(avgerror, axis=1)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## error margin on various ranges\n",
    "evaluation_accuracies = [.2,.4,.8,1.2,2,2.5,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## froming the unique name for the index values\n",
    "model_name = 'q3-depthmap-plaincnn-height-100-95k'\n",
    "run_no ='_front_run_03'\n",
    "complete_name = model_name+run_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating accuracies across the scantypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_360'\n",
    "full_model_name = complete_name + Scan\n",
    "rot = MAE.iloc[MAE.index.get_level_values('scantype') == '101']\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_rot = rot[(rot['error']<=acc) & (rot['error']>=-acc)]\n",
    "    rot_accuracy = len(predicted_rot)/len(rot)*100\n",
    "    print(\"total accuracy {} for 360:{}\".format(acc,rot_accuracy))\n",
    "    accuracy_list.append(rot_accuracy)\n",
    "rotdata =pd.DataFrame(accuracy_list)\n",
    "rotdata =rotdata.T\n",
    "rotdata.columns = evaluation_accuracies\n",
    "rotdata.rename(index={0:full_model_name}, inplace=True)\n",
    "rotdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_back'\n",
    "full_model_name = complete_name + Scan\n",
    "back = MAE.iloc[MAE.index.get_level_values('scantype') == '102']\n",
    "print(len(back))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_back = back[(back['error']<=acc) & (back['error']>=-acc)]\n",
    "    back_accuracy = len(predicted_back)/len(back)*100\n",
    "    print(\"total accuracy {} for back:{}\".format(acc,back_accuracy))\n",
    "    accuracy_list.append(back_accuracy)\n",
    "backdata =pd.DataFrame(accuracy_list)\n",
    "backdata =backdata.T\n",
    "backdata.columns = evaluation_accuracies\n",
    "backdata.rename(index={0:full_model_name}, inplace=True)\n",
    "backdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_front'\n",
    "full_model_name = complete_name + Scan\n",
    "front= MAE.iloc[MAE.index.get_level_values('scantype') == '100']\n",
    "print(\"total front samples:\",len(front))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_front = front[(front['error']<=acc) & (front['error']>=-acc)]\n",
    "    front_accuracy = len(predicted_front)/len(front)*100\n",
    "    print(\"total accuracy {} for front:{}\".format(acc,front_accuracy))\n",
    "    accuracy_list.append(front_accuracy)\n",
    "frontdata =pd.DataFrame(accuracy_list)\n",
    "frontdata =frontdata.T\n",
    "frontdata.columns = evaluation_accuracies\n",
    "frontdata.rename(index={0:full_model_name}, inplace=True)\n",
    "frontdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_lyingfront'\n",
    "full_model_name = complete_name + Scan\n",
    "lyingfront= MAE.iloc[MAE.index.get_level_values('scantype') == '200']\n",
    "print(\"total lyingfront samples:\",len(lyingfront))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_front = lyingfront[(lyingfront['error']<=acc) & (lyingfront['error']>=-acc)]\n",
    "    front_accuracy = len(predicted_front)/len(lyingfront)*100\n",
    "    print(\"total accuracy {} for lyingfront:{}\".format(acc,front_accuracy))\n",
    "    accuracy_list.append(front_accuracy)\n",
    "lyingfrontdata =pd.DataFrame(accuracy_list)\n",
    "lyingfrontdata =lyingfrontdata.T\n",
    "lyingfrontdata.columns = evaluation_accuracies\n",
    "lyingfrontdata.rename(index={0:full_model_name}, inplace=True)\n",
    "lyingfrontdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_lyingrot'\n",
    "full_model_name = complete_name + Scan\n",
    "lyingrot= MAE.iloc[MAE.index.get_level_values('scantype') == '201']\n",
    "print(\"total lyingrot samples:\",len(lyingrot))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_front = lyingrot[(lyingrot['error']<=acc) & (lyingrot['error']>=-acc)]\n",
    "    front_accuracy = len(predicted_front)/len(lyingrot)*100\n",
    "    print(\"total accuracy {} for lyingfront:{}\".format(acc,front_accuracy))\n",
    "    accuracy_list.append(front_accuracy)\n",
    "lyingrotdata =pd.DataFrame(accuracy_list)\n",
    "lyingrotdata =lyingrotdata.T\n",
    "lyingrotdata.columns = evaluation_accuracies\n",
    "lyingrotdata.rename(index={0:full_model_name}, inplace=True)\n",
    "lyingrotdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scan = '_lyingback'\n",
    "full_model_name = complete_name + Scan\n",
    "lyingback= MAE.iloc[MAE.index.get_level_values('scantype') == '202']\n",
    "print(\"total lyingback samples:\",len(lyingback))\n",
    "accuracy_list =[]\n",
    "for acc in evaluation_accuracies:\n",
    "    predicted_back = lyingback[(lyingback['error']<=acc) & (lyingback['error']>=-acc)]\n",
    "    back_accuracy = len(predicted_back)/len(lyingback)*100\n",
    "    print(\"total accuracy {} for lyingback:{}\".format(acc,back_accuracy))\n",
    "    accuracy_list.append(back_accuracy)\n",
    "lyingbackdata =pd.DataFrame(accuracy_list)\n",
    "lyingbackdata =lyingbackdata.T\n",
    "lyingbackdata.columns = evaluation_accuracies\n",
    "lyingbackdata.rename(index={0:full_model_name}, inplace=True)\n",
    "lyingbackdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining the results for all accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = pd.concat([frontdata, rotdata,backdata,lyingfrontdata,lyingrotdata,lyingbackdata])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.index.name = 'Model_Scantype'\n",
    "# alldata.set_names('Model', level=None, inplace=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model results in csv file\n",
    "result.to_csv('evaluation_95k_30082020/q3-depthmap-plaincnn-height-95k/run_03.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results =pd.concat([run_03,run_04,front])\n",
    "complete_results = complete_results.round(2) ## saving the results upto two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results.to_csv('Model_results/95k_data_v0/results.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
